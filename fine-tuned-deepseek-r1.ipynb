{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install unsloth # install unsloth\n",
    "!pip install --force-reinstall --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git # Also get the latest version Unsloth!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8leyDEWc7ZzP"
   },
   "outputs": [],
   "source": [
    "# Step3: Import necessary libraries\n",
    "import unsloth\n",
    "\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "from trl import SFTTrainer\n",
    "from unsloth import is_bfloat16_supported\n",
    "from huggingface_hub import login\n",
    "from transformers import TrainingArguments\n",
    "from datasets import load_dataset\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j3M8ft0FN_Zz"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TRITON_DISABLE_LINE_INFO\"] = \"1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ONMygdbK7dDY"
   },
   "outputs": [],
   "source": [
    "# # Step4: Check HF token\n",
    "# from google.colab import userdata\n",
    "# hf_token = userdata.get('HF_TOKEN')\n",
    "# login(hf_token)\n",
    "\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "from huggingface_hub import login\n",
    "import wandb\n",
    "\n",
    "user_secrets = UserSecretsClient()\n",
    "\n",
    "# Get Hugging Face token\n",
    "hf_token = user_secrets.get_secret(\"HF_TOKEN\")\n",
    "login(hf_token)\n",
    "\n",
    "# # Get WandB token\n",
    "# wandb_token = user_secrets.get_secret(\"WANDB_API_TOKEN\")\n",
    "# wandb.login(key=wandb_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x40Pxw8P7c_4",
    "outputId": "a87d5dab-b77c-4e8f-f6c7-23bfb54134dc"
   },
   "outputs": [],
   "source": [
    "# Optional: Check GPU availability\n",
    "# Test if CUDA is available\n",
    "import torch\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"GPU device:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bbtvM56n7c9j",
    "outputId": "9b52f955-6765-497f-e38b-be118ada67af"
   },
   "outputs": [],
   "source": [
    "# Step5: Setup pretrained DeepSeek-R1\n",
    "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"\n",
    "max_sequence_length = 2048\n",
    "dtype = None\n",
    "load_in_4bit = True\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = model_name,\n",
    "    max_seq_length = max_sequence_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    token = hf_token\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VB30wGRU7c7d"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Step6: Setup system prompt\n",
    "prompt_style = \"\"\"\n",
    "Below is a task description along with additional context provided in the input section. Your goal is to provide a well-reasoned response that effectively addresses the request.\n",
    "\n",
    "Before crafting your answer, take a moment to carefully analyze the question. Develop a clear, step-by-step thought process to ensure your response is both logical and accurate.\n",
    "\n",
    "### Task:\n",
    "You are a medical expert specializing in clinical reasoning, diagnostics, and treatment planning. Answer the medical question below using your advanced knowledge.\n",
    "\n",
    "### Query:\n",
    "{}\n",
    "\n",
    "### Answer:\n",
    "{}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n0GpFgQC7c5b",
    "outputId": "b4a4c4ad-9001-455a-ca68-250dbd25163a"
   },
   "outputs": [],
   "source": [
    "# Step7: Run Inference on the model\n",
    "\n",
    "# Define a test question\n",
    "question = \"\"\"A 61-year-old woman with a long history of involuntary urine loss during activities like coughing or\n",
    "              sneezing but no leakage at night undergoes a gynecological exam and Q-tip test. Based on these findings,\n",
    "              what would cystometry most likely reveal about her residual volume and detrusor contractions?\"\"\"\n",
    "\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "# Tokenize the input\n",
    "inputs = tokenizer([prompt_style.format(question, \"\")], return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# Generate a response\n",
    "outputs = model.generate (\n",
    "    input_ids = inputs.input_ids,\n",
    "    attention_mask = inputs.attention_mask,\n",
    "    max_new_tokens = 1200,\n",
    "    use_cache = True\n",
    ")\n",
    "\n",
    "# Decode the response tokens back to text\n",
    "response = tokenizer.batch_decode(outputs)\n",
    "\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jfBpUI5M7c3G",
    "outputId": "04d03e1f-b480-4524-ffe5-15cbfbf07324"
   },
   "outputs": [],
   "source": [
    "print(response[0].split(\"### Answer:\")[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iOyeh26b7vL1"
   },
   "outputs": [],
   "source": [
    "# Step8: Setup fine-tuning\n",
    "\n",
    "# Load Dataset\n",
    "medical_dataset = load_dataset(\"FreedomIntelligence/medical-o1-reasoning-SFT\", \"en\", split = \"train[:500]\", trust_remote_code = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "BCIh0TFl7vIq",
    "outputId": "749ccf71-5cc4-4cc1-f341-090680682001"
   },
   "outputs": [],
   "source": [
    "EOS_TOKEN = tokenizer.eos_token  # Define EOS_TOKEN which tells the model when to stop generating text during training\n",
    "EOS_TOKEN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AJb5a8Y77vGt"
   },
   "outputs": [],
   "source": [
    "### Finetuning\n",
    "# Updated training prompt style to add  tag\n",
    "train_prompt_style = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context.\n",
    "Write a response that appropriately completes the request.\n",
    "Before answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\n",
    "\n",
    "### Instruction:\n",
    "You are a medical expert with advanced knowledge in clinical reasoning, diagnostics, and treatment planning.\n",
    "Please answer the following medical question.\n",
    "\n",
    "### Question:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "\n",
    "{}\n",
    "\n",
    "{}\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wyjz-Xc-7vEh"
   },
   "outputs": [],
   "source": [
    "# Prepare the data for fine-tuning\n",
    "\n",
    "def preprocess_input_data(examples):\n",
    "  inputs = examples[\"Question\"]\n",
    "  cots = examples[\"Complex_CoT\"]\n",
    "  outputs = examples[\"Response\"]\n",
    "\n",
    "  texts = []\n",
    "\n",
    "  for input, cot, output in zip(inputs, cots, outputs):\n",
    "    text = train_prompt_style.format(input, cot, output) + EOS_TOKEN\n",
    "    texts.append(text)\n",
    "\n",
    "  return {\n",
    "      \"texts\" : texts,\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z4StgaW77vCN"
   },
   "outputs": [],
   "source": [
    "finetune_dataset = medical_dataset.map(preprocess_input_data, batched = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N9mdloTaCc4u",
    "outputId": "c82aea16-330f-49ff-92b0-9b91729066dd"
   },
   "outputs": [],
   "source": [
    "finetune_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dGTYEzvN7u_s"
   },
   "outputs": [],
   "source": [
    "# Step9: Setup/Apply LoRA finetuning to the model\n",
    "\n",
    "model_lora = FastLanguageModel.get_peft_model(\n",
    "    model = model,\n",
    "    r = 16,\n",
    "    target_modules = [\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\"\n",
    "    ],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0,\n",
    "    bias = \"none\",\n",
    "    use_gradient_checkpointing = \"unsloth\",\n",
    "    random_state = 3047,\n",
    "    use_rslora = False,\n",
    "    loftq_config = None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MfftPsxF7u9W"
   },
   "outputs": [],
   "source": [
    "# Add this before creating the trainer\n",
    "if hasattr(model, '_unwrapped_old_generate'):\n",
    "    del model._unwrapped_old_generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sM4mJTGPEHto"
   },
   "outputs": [],
   "source": [
    "# def formatting_func(example):\n",
    "#     # Assuming your dataset has 'instruction' and 'response' fields\n",
    "#     text = f\"### Instruction:\\n{example['instruction']}\\n### Response:\\n{example['response']}\"\n",
    "#     return [text]\n",
    "\n",
    "def formatting_func(examples):\n",
    "  inputs = examples[\"Question\"]\n",
    "  cots = examples[\"Complex_CoT\"]\n",
    "  outputs = examples[\"Response\"]\n",
    "\n",
    "  texts = []\n",
    "\n",
    "  for input, cot, output in zip(inputs, cots, outputs):\n",
    "    text = train_prompt_style.format(input, cot, output) + EOS_TOKEN\n",
    "    texts.append(text)\n",
    "\n",
    "  return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "9e7dbf664921466da0bc9a0972d38bd0",
      "2151ed39d72c49538260903c8a53bc5d",
      "eda739bd20504729a6a6be292c8232fb",
      "fa415a721dbb421fa37a415ec8b00142",
      "bd90043066d34e9aadfbf6b3f4a92f71",
      "c88d0719448b4eae885665e76bf0e8d6",
      "49e332e181e741c6af1df52e433f8e47",
      "73db672182d3414bba4a6c67e8c40353",
      "892977e303cc4418b3b59fc16f2d086e",
      "e808542eec354078a0bc981feba665e5",
      "a27c12f268c5463da69c94434481e214"
     ]
    },
    "id": "yfQyDY9T7u7J",
    "outputId": "44466f40-3cd1-4711-a97b-07ae7af5d430"
   },
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model = model_lora,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = finetune_dataset,\n",
    "    formatting_func=formatting_func,\n",
    "    dataset_text_field = \"texts\",\n",
    "    max_seq_length = max_sequence_length,\n",
    "    dataset_num_proc = 2,\n",
    "\n",
    "    # Define training args\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        num_train_epochs = 1,\n",
    "        warmup_steps = 5,\n",
    "        max_steps = 60,\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16=is_bfloat16_supported(),\n",
    "        logging_steps = 10,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        seed=3407,\n",
    "        output_dir = \"outputs\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 245
    },
    "id": "W745EhIJ7u4z",
    "outputId": "acc09c9e-1696-4d3e-c80f-ba6de8d4ae75"
   },
   "outputs": [],
   "source": [
    "# Setup WANDB\n",
    "wandb_token = user_secrets.get_secret(\"WANDB_API_TOKEN\")\n",
    "# login to wandb\n",
    "wandb.login(key=wandb_token)\n",
    "run = wandb.init(\n",
    "    project='Fine-tune-DeepSeek-R1-on-Medical-CoT-Dataset',\n",
    "    job_type=\"training\",\n",
    "    anonymous=\"allow\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ki_2rkHRNgRm"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TRITON_DISABLE_LINE_INFO\"] = \"1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 405
    },
    "id": "CXTkZURY7u2p",
    "outputId": "1079835e-0c68-4e5a-c0a1-e320f642b8e8"
   },
   "outputs": [],
   "source": [
    "# Start the fine-tuning process\n",
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wOSmKR0e7u0I"
   },
   "outputs": [],
   "source": [
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "63_Texlc7uvr"
   },
   "outputs": [],
   "source": [
    "# Step10: Testing after fine-tuning\n",
    "question = \"\"\"A 61-year-old woman with a long history of involuntary urine loss during activities like coughing or sneezing\n",
    "              but no leakage at night undergoes a gynecological exam and Q-tip test. Based on these findings,\n",
    "              what would cystometry most likely reveal about her residual volume and detrusor contractions?\"\"\"\n",
    "\n",
    "FastLanguageModel.for_inference(model_lora)\n",
    "\n",
    "# Tokenize the input\n",
    "inputs = tokenizer([prompt_style.format(question, \"\")], return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# Generate a response\n",
    "outputs = model_lora.generate (\n",
    "    input_ids = inputs.input_ids,\n",
    "    attention_mask = inputs.attention_mask,\n",
    "    max_new_tokens = 1200,\n",
    "    use_cache = True\n",
    ")\n",
    "\n",
    "# Decode the response tokens back to text\n",
    "response = tokenizer.batch_decode(outputs)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fumiPBu_7utl"
   },
   "outputs": [],
   "source": [
    "\n",
    "print(response[0].split(\"### Answer:\")[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"\"\"A 59-year-old man presents with a fever, chills, night sweats, and generalized fatigue,\n",
    "              and is found to have a 12 mm vegetation on the aortic valve. Blood cultures indicate gram-positive, catalase-negative,\n",
    "              gamma-hemolytic cocci in chains that do not grow in a 6.5% NaCl medium.\n",
    "              What is the most likely predisposing factor for this patient's condition?\"\"\"\n",
    "\n",
    "FastLanguageModel.for_inference(model_lora)\n",
    "\n",
    "# Tokenize the input\n",
    "inputs = tokenizer([prompt_style.format(question, \"\")], return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# Generate a response\n",
    "outputs = model_lora.generate (\n",
    "    input_ids = inputs.input_ids,\n",
    "    attention_mask = inputs.attention_mask,\n",
    "    max_new_tokens = 1200,\n",
    "    use_cache = True\n",
    ")\n",
    "\n",
    "# Decode the response tokens back to text\n",
    "response = tokenizer.batch_decode(outputs)\n",
    "\n",
    "print(response[0].split(\"### Answer:\")[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
